{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b299cc60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9HCLKkbHzrt",
    "outputId": "b7efdf13-aaba-4354-82ba-a2262144c0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e041c18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lezWe04nHzr0",
    "outputId": "8a2c4a73-2882-4437-c8d3-b0ea0239cc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d6611c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwpx3I0iHzr1",
    "outputId": "f10352e5-9286-4703-8b92-63ac40578db2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere Enhance Documents in One Click \n",
      "Collaborate With Others With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the ‘Storage’ \n",
      "option, we’ll also store all processed files here. \n",
      "You can access files stored on Smallpdf from \n",
      "your computer, phone, or tablet. We’ll also \n",
      "sync files from the Smallpdf Mobile App to our \n",
      "online portalWhen you right-click on a file, we’ll present \n",
      "you with an array of options to convert, \n",
      "compress, or modify it. \n",
      "Forget mundane administrative tasks. With \n",
      "Smallpdf, you can request e-signatures, send \n",
      "large files, or even enable the Smallpdf G Suite \n",
      "App for your entire organization. Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# creating a pdf reader object\n",
    "with open(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\DSBDAL\\Ex7 text_analysis\\sample1.pdf\", 'rb') as pdfFileObj:\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PdfReader(pdfFileObj)\n",
    "    \n",
    "    # printing number of pages in pdf file\n",
    "    print(len(pdfReader.pages))\n",
    "    \n",
    "    # extracting text from page\n",
    "    pageObj = pdfReader.pages[0]\n",
    "    print(pageObj.extract_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e8495d",
   "metadata": {
    "id": "9OqmTpY4Hzr3"
   },
   "outputs": [],
   "source": [
    "# import docx NOT python-docx\n",
    "import docx\n",
    "  \n",
    "# create an instance of a word document\n",
    "doc = docx.Document()\n",
    "  \n",
    "# add a heading of level 0 (largest heading)\n",
    "doc.add_heading('Heading for the document', 0)\n",
    "  \n",
    "# add a paragraph and store \n",
    "# the object in a variable\n",
    "doc_para = doc.add_paragraph('Your paragraph goes here, ')\n",
    "  \n",
    "# add a run i.e, style like \n",
    "# bold, italic, underline, etc.\n",
    "doc_para.add_run('hey there, bold here').bold = True\n",
    "doc_para.add_run(', and ')\n",
    "doc_para.add_run('these words are italic').italic = True\n",
    "  \n",
    "# add a page break to start a new page\n",
    "doc.add_page_break()\n",
    "  \n",
    "# add a heading of level 2\n",
    "doc.add_heading('Heading level 2', 2)\n",
    "  \n",
    "# pictures can also be added to our word document\n",
    "# width is optional\n",
    "doc.add_picture(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\DSBDAL\\Ex7 text_analysis\\index.jpg\")\n",
    "  \n",
    "# now save the document to a location\n",
    "doc.save('new_doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f4a557",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVbi3BHwHzr4",
    "outputId": "0028cd1a-6594-49b1-e8d0-73a5b3d6aadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afb3e46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdnoGOs1Hzr5",
    "outputId": "1681934c-db0a-4783-83ae-a0ff4b1cdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d79ff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdJULL3IHzr6",
    "outputId": "2ed680be-a0ad-4f5f-9ca5-4c991216ac4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72819a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFSoVWabHzr7",
    "outputId": "45e190f9-fa6a-4c5f-a421-4cde12e7312d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "#Non English language Tokenization\n",
    "\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53fd94ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwNd_fYoHzr8",
    "outputId": "8aa8f23a-2b44-4d95-962d-d9a98741fbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fd6896",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5NXxeG4Hzr9",
    "outputId": "21e90076-8ec2-474b-f1ac-3aa119f89b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'boy', '.', 'She', 'is', 'a', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "  \n",
    "#Dummy text\n",
    "txt = \"He is a boy. \"\\\n",
    "    \"She is a girl\"\n",
    "\n",
    "word_tokens = word_tokenize(txt)\n",
    "  \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5548ae14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1q_7aeTHzr-",
    "outputId": "53050fa9-b159-4b78-a8a9-472bcd8f5775"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('welcome', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('Categorizing', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagging', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Python', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speech (POS) tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1023572a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7JOBlU3Hzr_",
    "outputId": "1a37d5d5-4ac8-420a-de88-f3083bf76711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82130d42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdRww8lTHzsA",
    "outputId": "97798dff-47f3-44b6-8c29-ac76391697b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd66b76c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-v8o9aRHzsA",
    "outputId": "b12abe0d-dc63-455d-e834-62c33b52efaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Stop Words Removed: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stopwords removal from sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(\"Tokenized:\", word_tokens)\n",
    "print(\"Stop Words Removed:\", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85db980",
   "metadata": {
    "id": "wbAKWeX0HzsC"
   },
   "outputs": [],
   "source": [
    "#Stopwords from input file\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\DSBDAL\\Ex7 text_analysis\\text.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af73402",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooyFq1aJHzsC",
    "outputId": "7b8b00f0-40b9-4bc6-dd26-018792994a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: it\n",
      "Actual: vijaying  Stem: vijay\n",
      "Actual: meeting  Stem: meet\n",
      "Actual: better  Stem: better\n",
      "Actual: vijayed  Stem: vijay\n",
      "Actual: vijays  Stem: vijay\n",
      "Actual: eats  Stem: eat\n",
      "Actual: skills  Stem: skill\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It vijaying meeting better vijayed vijays eats skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a9b5fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSmXKl9MHzsD",
    "outputId": "bb510e80-fcbe-4748-887c-5bb4339cc916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lemma: It\n",
      "Actual: studies  Lemma: study\n",
      "Actual: densely  Lemma: densely\n",
      "Actual: is  Lemma: is\n",
      "Actual: better  Lemma: better\n",
      "Actual: meeting  Lemma: meeting\n",
      "Actual: studying  Lemma: studying\n",
      "Actual: vijaying  Lemma: vijaying\n",
      "Actual: vijayed  Lemma: vijayed\n",
      "Actual: vijays  Lemma: vijays\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"It studies densely is better  meeting studying vijaying vijayed vijays skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "        print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c740c3d",
   "metadata": {
    "id": "J8SNEOlSHzsD"
   },
   "outputs": [],
   "source": [
    "#Expt.No.7 2nd Operation\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919c8b3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJyLNJNEHzsD",
    "outputId": "9554d79c-d3d0-4b15-cf6a-defd5ae308b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data', 'learning', 'of', 'century', '21st', 'science', 'data', 'the', 'best', 'key', 'Science', 'is', 'Machine', 'for', 'job'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the best job of the 21st century\"\n",
    "second_sentence = \"Machine learning is the key for data science\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8a2b721",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7_Op7_EHHzsE",
    "outputId": "65c157ae-3735-454d-fc95-ff6f41f7da48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>learning</th>\n",
       "      <th>of</th>\n",
       "      <th>century</th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>key</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>Machine</th>\n",
       "      <th>for</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data  learning  of  century  21st  science  data  the  best  key  Science  \\\n",
       "0     1         0   1        1     1        0     0    2     1    0        1   \n",
       "1     0         1   0        0     0        1     1    1     0    1        0   \n",
       "\n",
       "   is  Machine  for  job  \n",
       "0   1        0    0    1  \n",
       "1   1        1    1    0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the words\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9886a78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "MxENejj3HzsE",
    "outputId": "6d95205b-1b6c-4703-e51e-a6e6d604c326"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>learning</th>\n",
       "      <th>of</th>\n",
       "      <th>century</th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>key</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>Machine</th>\n",
       "      <th>for</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data  learning   of  century  21st  science   data    the  best    key  \\\n",
       "0   0.1     0.000  0.1      0.1   0.1    0.000  0.000  0.200   0.1  0.000   \n",
       "1   0.0     0.125  0.0      0.0   0.0    0.125  0.125  0.125   0.0  0.125   \n",
       "\n",
       "   Science     is  Machine    for  job  \n",
       "0      0.1  0.100    0.000  0.000  0.1  \n",
       "1      0.0  0.125    0.125  0.125  0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute Term Frequency(TF)\n",
    "\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50a0388c",
   "metadata": {
    "id": "L6mbtNcrHzsF"
   },
   "outputs": [],
   "source": [
    "#Compute Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d971f33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "QhZ1RQt3HzsF",
    "outputId": "2d1a71c7-21b3-4d73-f1fb-91b2874c92e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>learning</th>\n",
       "      <th>of</th>\n",
       "      <th>century</th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>key</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>Machine</th>\n",
       "      <th>for</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Data  learning        of   century      21st   science      data  \\\n",
       "0  0.030103  0.000000  0.030103  0.030103  0.030103  0.000000  0.000000   \n",
       "1  0.000000  0.037629  0.000000  0.000000  0.000000  0.037629  0.037629   \n",
       "\n",
       "        the      best       key   Science        is   Machine       for  \\\n",
       "0  0.060206  0.030103  0.000000  0.030103  0.030103  0.000000  0.000000   \n",
       "1  0.037629  0.000000  0.037629  0.000000  0.037629  0.037629  0.037629   \n",
       "\n",
       "        job  \n",
       "0  0.030103  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute Term Frequency(TF) - Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679a4f64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-6YcxsJHzsG",
    "outputId": "8c51d011-4a07-4231-df8c-35bf1a90ca97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34211869506421816\n",
      "  (0, 0)\t0.34211869506421816\n",
      "  (0, 9)\t0.34211869506421816\n",
      "  (0, 5)\t0.34211869506421816\n",
      "  (0, 11)\t0.34211869506421816\n",
      "  (0, 12)\t0.48684053853849035\n",
      "  (0, 4)\t0.24342026926924518\n",
      "  (0, 10)\t0.24342026926924518\n",
      "  (0, 2)\t0.24342026926924518\n",
      "  (1, 3)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 10)\t0.28986933576883284\n",
      "  (1, 2)\t0.28986933576883284\n"
     ]
    }
   ],
   "source": [
    "#Compute TF-IDF\n",
    "\n",
    "#first step is to import the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee881966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSure, let\\'s go through the provided code step by step:\\n\\n1. **Install Dependencies:**\\n   - Install the `PyPDF2` and `python-docx` packages using pip.\\n\\n2. **Extract Text from PDF:**\\n   - Use PyPDF2 to extract text from a PDF file (`sample1.pdf`).\\n   - Open the PDF file in binary mode (`\\'rb\\'`).\\n   - Print the number of pages in the PDF and extract text from the first page.\\n\\n3. **Create and Manipulate Word Document:**\\n   - Import `docx`.\\n   - Create a Word document object.\\n   - Add a heading, paragraph, and styled runs to the document.\\n   - Add a page break and another heading.\\n   - Add a picture to the document.\\n   - Save the document as `new_doc.docx`.\\n\\n4. **Install and Download NLTK Resources:**\\n   - Install NLTK (`pip install nltk`).\\n   - Import NLTK and download necessary resources using `nltk.download()` and `nltk.download(\\'punkt\\')`.\\n\\n5. **Tokenization:**\\n   - Tokenize sentences (`nltk.sent_tokenize()`) and words (`nltk.word_tokenize()`).\\n   - Perform sentence tokenization and word tokenization for English and German languages.\\n\\n6. **Part of Speech Tagging (POS):**\\n   - Perform Part of Speech tagging (`nltk.pos_tag()`).\\n\\n7. **Stopwords Removal:**\\n   - Print a list of English stopwords (`stopwords.words(\\'english\\')`).\\n   - Remove stopwords from a given sentence.\\n\\n8. **Stopwords Removal from Input File:**\\n   - Read a text file (`text.txt`).\\n   - Remove stopwords and write the filtered text to a new file (`filteredtext.txt`).\\n\\n9. **Stemming:**\\n   - Use Porter Stemmer to find the stems of words in a sentence.\\n\\n10. **Lemmatization:**\\n    - Use WordNet Lemmatizer to lemmatize words in a sentence.\\n\\n11. **Data Analysis Operations:**\\n    - Import pandas, sklearn, and math.\\n    - Define two sentences.\\n    - Perform word count, TF (Term Frequency), IDF (Inverse Document Frequency), and TF-IDF (Term Frequency-Inverse Document Frequency) computations.\\n\\n12. **TF-IDF using sklearn:**\\n    - Import `TfidfVectorizer` from sklearn.\\n    - Use `TfidfVectorizer` to compute TF-IDF scores for the provided sentences.\\n\\nEach section of code serves different purposes, from text extraction, manipulation, and analysis to natural language processing tasks such as tokenization, part of speech tagging, stopwords removal, stemming, and lemmatization.\\n\\nDefinition: Tokenization is the process of breaking down a text into smaller units called tokens, which can be words, phrases, symbols, or other meaningful elements.\\nPurpose: It provides a way to split text into manageable units for further analysis.\\nExample: Sentence \"Tokenization is important!\" tokenized into [\"Tokenization\", \"is\", \"important\", \"!\"].\\nPart-of-Speech (POS) Tagging:\\nDefinition: POS tagging involves assigning grammatical categories (e.g., noun, verb, adjective) to each token in a text.\\nPurpose: It helps in understanding the grammatical structure of sentences, which is crucial for many NLP tasks.\\nExample: Tagging \"Tokenization is important!\" as [(Tokenization, Noun), (is, Verb), (important, Adjective), (!, Punctuation)].\\nStop Words Removal:\\nDefinition: Stop words are common words (e.g., \"is\", \"the\", \"and\") that are often removed from text data because they typically do not contribute much to the meaning of the text.\\nPurpose: Removing stop words can improve the efficiency of text processing and prevent them from affecting downstream tasks.\\nExample: Removing \"is\", \"the\", and \"and\" from a sentence.\\nStemming:\\nDefinition: Stemming is the process of reducing words to their root or base form by removing suffixes.\\nPurpose: It aims to normalize words with the same meaning to a common form, which can improve text analysis.\\nExample: Stemming \"running\", \"runs\", and \"runner\" to the root \"run\".\\nLemmatization:\\nDefinition: Lemmatization is similar to stemming but produces valid words (lemmas) by considering the word\\'s context and morphological analysis.\\nPurpose: It helps in reducing words to their dictionary form, preserving the meaning of words better than stemming.\\nExample: Lemmatizing \"running\", \"runs\", and \"runner\" to the lemma \"run\".\\n\\nTerm Frequency measures the frequency of a term (word) within a document.\\nInverse Document Frequency measures the importance of a term across a corpus of documents by penalizing terms that occur frequently across documents.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sure, let's go through the provided code step by step:\n",
    "\n",
    "1. **Install Dependencies:**\n",
    "   - Install the `PyPDF2` and `python-docx` packages using pip.\n",
    "\n",
    "2. **Extract Text from PDF:**\n",
    "   - Use PyPDF2 to extract text from a PDF file (`sample1.pdf`).\n",
    "   - Open the PDF file in binary mode (`'rb'`).\n",
    "   - Print the number of pages in the PDF and extract text from the first page.\n",
    "\n",
    "3. **Create and Manipulate Word Document:**\n",
    "   - Import `docx`.\n",
    "   - Create a Word document object.\n",
    "   - Add a heading, paragraph, and styled runs to the document.\n",
    "   - Add a page break and another heading.\n",
    "   - Add a picture to the document.\n",
    "   - Save the document as `new_doc.docx`.\n",
    "\n",
    "4. **Install and Download NLTK Resources:**\n",
    "   - Install NLTK (`pip install nltk`).\n",
    "   - Import NLTK and download necessary resources using `nltk.download()` and `nltk.download('punkt')`.\n",
    "\n",
    "5. **Tokenization:**\n",
    "   - Tokenize sentences (`nltk.sent_tokenize()`) and words (`nltk.word_tokenize()`).\n",
    "   - Perform sentence tokenization and word tokenization for English and German languages.\n",
    "\n",
    "6. **Part of Speech Tagging (POS):**\n",
    "   - Perform Part of Speech tagging (`nltk.pos_tag()`).\n",
    "\n",
    "7. **Stopwords Removal:**\n",
    "   - Print a list of English stopwords (`stopwords.words('english')`).\n",
    "   - Remove stopwords from a given sentence.\n",
    "\n",
    "8. **Stopwords Removal from Input File:**\n",
    "   - Read a text file (`text.txt`).\n",
    "   - Remove stopwords and write the filtered text to a new file (`filteredtext.txt`).\n",
    "\n",
    "9. **Stemming:**\n",
    "   - Use Porter Stemmer to find the stems of words in a sentence.\n",
    "\n",
    "10. **Lemmatization:**\n",
    "    - Use WordNet Lemmatizer to lemmatize words in a sentence.\n",
    "\n",
    "11. **Data Analysis Operations:**\n",
    "    - Import pandas, sklearn, and math.\n",
    "    - Define two sentences.\n",
    "    - Perform word count, TF (Term Frequency), IDF (Inverse Document Frequency), and TF-IDF (Term Frequency-Inverse Document Frequency) computations.\n",
    "\n",
    "12. **TF-IDF using sklearn:**\n",
    "    - Import `TfidfVectorizer` from sklearn.\n",
    "    - Use `TfidfVectorizer` to compute TF-IDF scores for the provided sentences.\n",
    "\n",
    "Each section of code serves different purposes, from text extraction, manipulation, and analysis to natural language processing tasks such as tokenization, part of speech tagging, stopwords removal, stemming, and lemmatization.\n",
    "\n",
    "Definition: Tokenization is the process of breaking down a text into smaller units called tokens, which can be words, phrases, symbols, or other meaningful elements.\n",
    "Purpose: It provides a way to split text into manageable units for further analysis.\n",
    "Example: Sentence \"Tokenization is important!\" tokenized into [\"Tokenization\", \"is\", \"important\", \"!\"].\n",
    "Part-of-Speech (POS) Tagging:\n",
    "Definition: POS tagging involves assigning grammatical categories (e.g., noun, verb, adjective) to each token in a text.\n",
    "Purpose: It helps in understanding the grammatical structure of sentences, which is crucial for many NLP tasks.\n",
    "Example: Tagging \"Tokenization is important!\" as [(Tokenization, Noun), (is, Verb), (important, Adjective), (!, Punctuation)].\n",
    "Stop Words Removal:\n",
    "Definition: Stop words are common words (e.g., \"is\", \"the\", \"and\") that are often removed from text data because they typically do not contribute much to the meaning of the text.\n",
    "Purpose: Removing stop words can improve the efficiency of text processing and prevent them from affecting downstream tasks.\n",
    "Example: Removing \"is\", \"the\", and \"and\" from a sentence.\n",
    "Stemming:\n",
    "Definition: Stemming is the process of reducing words to their root or base form by removing suffixes.\n",
    "Purpose: It aims to normalize words with the same meaning to a common form, which can improve text analysis.\n",
    "Example: Stemming \"running\", \"runs\", and \"runner\" to the root \"run\".\n",
    "Lemmatization:\n",
    "Definition: Lemmatization is similar to stemming but produces valid words (lemmas) by considering the word's context and morphological analysis.\n",
    "Purpose: It helps in reducing words to their dictionary form, preserving the meaning of words better than stemming.\n",
    "Example: Lemmatizing \"running\", \"runs\", and \"runner\" to the lemma \"run\".\n",
    "\n",
    "Term Frequency measures the frequency of a term (word) within a document.\n",
    "Inverse Document Frequency measures the importance of a term across a corpus of documents by penalizing terms that occur frequently across documents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d9631-17af-4e3f-b296-921e5b6545cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Expt.No.7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
